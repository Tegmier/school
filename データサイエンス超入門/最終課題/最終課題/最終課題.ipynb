{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb07a911",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Import the moduels</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df4beccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "from scipy import spatial\n",
    "from pyvis.network import Network\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "import numpy as np\n",
    "import pyathena\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from googleapiclient.discovery import build\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "080b315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KKey Settings\n",
    "apis = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\apis.csv\")\n",
    "openai.api_key = apis[\"openai\"][0]\n",
    "API_KEY = apis[\"googlesearch\"][0]\n",
    "CUSTOM_SEARCH_ENGINE = 'a78a2783d10a44be6'\n",
    "page_limit = 1\n",
    "PROMPT_TEXT_KADAI_CORP = '''\n",
    "    この文章の主題を15文字以上30文字未満で要約してください。\n",
    "'''\n",
    "\n",
    "PROMPT_TEXT_FC = '''\n",
    "    この文章の主題を要約すると次の5個です。それぞれ15文字以上30文字未満で要約してください。\n",
    "'''\n",
    "\n",
    "PROMPT_TEXT_OTR = '''\n",
    "    この文章の主題を要約すると次の3個です。それぞれ15文字以上30文字未満で要約してください。\n",
    "'''\n",
    "\n",
    "model = SentenceTransformer('stsb-xlm-r-multilingual')\n",
    "\n",
    "OUTPUT_TOKENS = 1000\n",
    "\n",
    "MAX_DISTANCE = 3.0\n",
    "EMPTY_TEXT = ''\n",
    "SIMILARITY_UPPER_THRESHOLD = 3.00\n",
    "SIMILARITY_LOWER_THRESHOLD = 2.65\n",
    "RESCALED_MAX_SIMILARITY_VALUE = 3.0\n",
    "RESCALED_MIN_SIMILARITY_VALUE = 0.1\n",
    "\n",
    "distance_fc_list = []\n",
    "distance_otr_list = []\n",
    "distance_fc_and_otr_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12979197",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Vector Embeding/Text Processing Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5a5d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageUrl(api_key, cse_key, search_word):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    page_limit = 1\n",
    "    startIndex = 1\n",
    "    response = []\n",
    "    img_list = []\n",
    "\n",
    "    try:\n",
    "        response.append(service.cse().list(\n",
    "            q=search_word,     # Search words\n",
    "            cx=cse_key,        # custom search engine key\n",
    "            lr='lang_ja',      # Search language\n",
    "            num=1,            # Number of images obtained by one request (Max 10)\n",
    "            start=startIndex,\n",
    "            searchType='image' # search for images\n",
    "        ).execute())\n",
    "\n",
    "        startIndex = response[0].get(\"queries\").get(\"nextPage\")[0].get(\"startIndex\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    for one_res in range(len(response)):\n",
    "        if int(response[one_res][\"searchInformation\"][\"totalResults\"]) > 0:\n",
    "            for i in range(len(response[one_res]['items'])):\n",
    "                img_list.append(response[one_res]['items'][i]['link'])\n",
    "        else:\n",
    "            img_list.append(\"https://1.bp.blogspot.com/-d3vDLBoPktU/WvQHWMBRhII/AAAAAAABL6E/Grg-XGzr9jEODAxkRcbqIXu-mFA9gTp3wCLcBGAs/s800/internet_404_page_not_found.png\")\n",
    "\n",
    "    return img_list[0]\n",
    "\n",
    "# Get the sentence Vectors\n",
    "def get_sentence_vector(in_sentence):\n",
    "    return model.encode(in_sentence, convert_to_tensor=False)\n",
    "\n",
    "def get_similarity(in_vec_a, in_vec_b):\n",
    "    return MAX_DISTANCE - spatial.distance.cosine(in_vec_a, in_vec_b)\n",
    "\n",
    "def _get_cleaned_text(in_text):\n",
    "    in_text = in_text.replace('　', ' ')\n",
    "    return in_text\n",
    "\n",
    "def _get_empty_text_removed_list(in_list):\n",
    "    return [s for s in in_list if s != EMPTY_TEXT]\n",
    "\n",
    "def _get_parsed_result_by_return(in_sentence):\n",
    "    return _get_empty_text_removed_list(in_sentence.split('\\n'))\n",
    "\n",
    "def _get_numbering_removed_keyword(in_keyword_list):\n",
    "    this_result = []\n",
    "    for this_keyword in in_keyword_list:\n",
    "        this_keyword = re.sub(r'^\\d+\\. ', '', this_keyword)\n",
    "        this_result.append(re.sub(r'^\\d+\\.', '', this_keyword))\n",
    "    return this_result\n",
    "\n",
    "def get_clustering_results_chat_completions(in_text, prompt):\n",
    "    print(in_text)\n",
    "    response = openai.ChatCompletion.create()\n",
    "    # print(response[\"choices\"])\n",
    "    print(prompt + in_text)\n",
    "    answer_text = response[\"choices\"][0][\"text\"]\n",
    "    answer_text_replaced = answer_text.replace(r'\\u', r'\\\\u')\n",
    "    print(answer_text_replaced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02976b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## clustering ##########################\n",
    "def sentence_vector_preprocessing_before_clustering(fc_df):\n",
    "    vectors = []\n",
    "    for i in range(len(fc_df)):\n",
    "        vectors.append(np.array(fc_df[\"Vector\"][i]))\n",
    "    norm_vectors = normalize(vectors)\n",
    "    cosine_sim_vectors = cosine_similarity(vectors)\n",
    "    return norm_vectors, cosine_sim_vectors\n",
    "\n",
    "\n",
    "def clustering_metrics_evaluation(input_vectors, fitted_labels, model_title, preprocess_method):\n",
    "    print(\"############# Displaying the statistical result of the \" + model_title + \"with preprocess method of \"+ preprocess_method + \" #############\")\n",
    "    silhousette = metrics.silhouette_score(input_vectors, fitted_labels)\n",
    "    print(\"The Silhousette Score: \", silhousette)\n",
    "\n",
    "    calinski = metrics.calinski_harabasz_score(input_vectors, fitted_labels)\n",
    "    print(\"The Calinski-Harabasz Score: \", calinski)\n",
    "\n",
    "    Davies = metrics.davies_bouldin_score(input_vectors, fitted_labels)\n",
    "    print(\"The Davies-Bouldin Score: \", Davies)\n",
    "\n",
    "def demonstrating_clustering_result(fitted_labels):\n",
    "    pass\n",
    "    # print(fitted_labels)\n",
    "    \n",
    "def clustering_Kmeans(fc_df, num_clusters, preprocess_method):\n",
    "    norm_vectors, cosine_sim_vectors = sentence_vector_preprocessing_before_clustering(fc_df)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    if preprocess_method =='normalization':\n",
    "        kmeans.fit(norm_vectors)\n",
    "    elif preprocess_method == 'cosine':\n",
    "        kmeans.fit(cosine_sim_vectors)\n",
    "    else:\n",
    "        fitted_labels = []\n",
    "        print(\"Wrong preprocess method!\")\n",
    "    fitted_labels = kmeans.labels_\n",
    "    fc_df[\"Cluster\"] = fitted_labels\n",
    "    \n",
    "    # Evaluation\n",
    "    clustering_metrics_evaluation(norm_vectors, fitted_labels, \"Kmeans_clustering\", preprocess_method)\n",
    "    demonstrating_clustering_result(fitted_labels)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9126cf",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Clustering Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f077defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def Agglomerative_Clustering(fc_df, num_clusters, preprocess_method):\n",
    "    norm_vectors, cosine_sim_vectors = sentence_vector_preprocessing_before_clustering(fc_df)\n",
    "    agglomerativeClustering = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "    if preprocess_method =='normalization':\n",
    "        agglomerativeClustering.fit(norm_vectors)\n",
    "    elif preprocess_method == 'cosine':\n",
    "        agglomerativeClustering.fit(cosine_sim_vectors)\n",
    "    else:\n",
    "        fitted_labels = []\n",
    "        print(\"Wrong preprocess method!\")\n",
    "\n",
    "    fitted_labels = agglomerativeClustering.labels_\n",
    "    fc_df[\"Cluster\"] = fitted_labels\n",
    "\n",
    "    # Evaluation\n",
    "    clustering_metrics_evaluation(norm_vectors, fitted_labels, \"Agglomerative_clustering\", preprocess_method)\n",
    "    demonstrating_clustering_result(fitted_labels)\n",
    "\n",
    "# def clustering_LLM(fc_df, num_clusters):\n",
    "#     clustering_prompt = PROMPT_CLUSTERING_1 + str(num_clusters) + PROMPT_CLUSTERING_2\n",
    "#     vector_in_text = '\\n'.join(f'{str(i)}.{fc_df[\"Keyword\"][i]}' for i in range(len(fc_df[\"Keyword\"])))\n",
    "#     # vector_in_text = '\\n'.join(f\"{keyword}\" for keyword in fc_df[\"Keyword\"])\n",
    "#     get_clustering_results_completions(vector_in_text, clustering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0190a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_for_one_text(in_text, prompt):\n",
    "    response = openai.Completion.create(\n",
    "        # model=\"gpt-4\",\n",
    "        # model=\"gpt-3.5-turbo\",\n",
    "        # model=\"text-davinci-003\",\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=_get_cleaned_text(in_text) + prompt,\n",
    "        temperature=0, #temperature=0.5,\n",
    "        max_tokens=OUTPUT_TOKENS,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.8,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    return _get_numbering_removed_keyword(\n",
    "        _get_parsed_result_by_return(\n",
    "            response[\"choices\"][0][\"text\"]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "874ffba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n",
      "                                             sentence            counterpart\n",
      "23  横浜市中央児童相談所の環境改善のための連携について（PDF：307KB）, 横浜市児童相談所...   横浜市こども青少年局中央児童相談所庶務係\n",
      "24  ＨＩＶ・エイズ、性感染症検査等の普及啓発（PDF：758KB）, ＨＩＶ／エイズの感染経路は...  横浜市健康福祉局健康安全課結核・エイズ担当\n",
      "25  「市民と企業によるまちづくり」\\r\\n協働による地域の課題解決・魅力向上のための施設（ハード...       横浜市都市整備局地域まちづくり課\n",
      "26  「横浜消防」のＰＲ・ブランディングと市民の防災・減災意識の啓発（PDF：329KB）, 大地...           横浜市消防局総務部企画課\n",
      "27  市民の読書活動の推進のための連携, 読書活動は言葉を学び、感性を磨き、表現力を高め、創造力を...    横浜市教育委員会事務局生涯学習文化財課\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "###################### 課題データセットの取得 ##############################\n",
    "###########################################################################\n",
    "kadai_df = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\dataleaves\\\\yokohama_kyoso_theme.csv\")\n",
    "kadai_df.columns = [\"kid\", \"theme\", \"gaiyou\", \"shokan\"]\n",
    "kadai_df[\"fulltext\"] = [f\"{tm}, {gy}\" for tm, gy in zip(kadai_df[\"theme\"], kadai_df[\"gaiyou\"])]\n",
    "sentences_kadai = []\n",
    "for add_s in kadai_df[\"fulltext\"]:\n",
    "    sentences_kadai.append(add_s.strip())\n",
    "    \n",
    "sentences_kadai_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_kadai\n",
    "})\n",
    "sentences_kadai_df[\"counterpart\"] = kadai_df[\"shokan\"]\n",
    "\n",
    "#Compute embeddings\n",
    "if False:\n",
    "    print(\"Start calc embeddings\")\n",
    "    print(datetime.datetime.now())\n",
    "    embeddings_kadai = model.encode(sentences_kadai, convert_to_tensor=True)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # pickle化してファイルに書き込み\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_kadai.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_kadai, f)\n",
    "else:\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_kadai.pkl', 'rb') as f:\n",
    "        embeddings_kadai = pickle.load(f)\n",
    "        \n",
    "print(len(embeddings_kadai))\n",
    "print(len(sentences_kadai_df))\n",
    "print(sentences_kadai_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e80656c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n",
      "466\n",
      "                                              sentence   counterpart\n",
      "461                       横浜市内を中心に公共工事、土木舗装工事を主としています。          和紘建設\n",
      "462                                              総合建設業           渡辺組\n",
      "463                                                nan          渡辺商事\n",
      "464  台風・地震・大雨等の天災時にも直ちに出動できる態勢を整えており、横浜市の“災害登録業者”にも...          綿貫建設\n",
      "465                                            化粧品の製造卸  ワミレスコスメティックス\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "###################### 企業データセットの取得 ##############################\n",
    "###########################################################################\n",
    "\n",
    "corp_df = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\dataleaves\\\\yokohama_chiikikouken.csv\")\n",
    "corp_df[\"fulltext\"] = [f\"{pr}, {jg}, {tk}\" for pr, jg, tk in zip(corp_df[\"PR\"], corp_df[\"jigyou\"], corp_df[\"torikumi\"])]\n",
    "corp_df[\"fulltext\"] = [st.replace(\"nan, \", \"\") for st in corp_df[\"fulltext\"]]\n",
    "corp_df[\"fulltext\"] = [st.replace(\", nan\", \"\") for st in corp_df[\"fulltext\"]]\n",
    "\n",
    "sentences_corp = []\n",
    "for add_s in corp_df[\"fulltext\"]:\n",
    "    sentences_corp.append(add_s.strip())\n",
    "    \n",
    "sentences_corp_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_corp\n",
    "})\n",
    "sentences_corp_df[\"counterpart\"] = [st.strip().replace(\"株式会社\", \"\").replace(\"ホールディングス\", \"\") for st in corp_df[\"corpname\"]]\n",
    "\n",
    "#Compute embeddings\n",
    "if False:\n",
    "    print(\"Start calc embeddings\")\n",
    "    print(datetime.datetime.now())\n",
    "    embeddings_corp = model.encode(sentences_corp, convert_to_tensor=True)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # pickle化してファイルに書き込み\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_corp.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_corp, f)\n",
    "else:\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_corp.pkl', 'rb') as f:\n",
    "        embeddings_corp = pickle.load(f)\n",
    "print(len(embeddings_corp))\n",
    "print(len(sentences_corp_df))\n",
    "print(sentences_corp_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "77a76671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "    data_number                             fulltext       institute  \\\n",
      "74           75  ('横浜市：2020年度 U39アーティスト・フェローシップ助成',)     横浜市芸術文化振興財団   \n",
      "75           76        ('横浜市：クリエイティブ・インクルージョン活動助成',)     横浜市芸術文化振興財団   \n",
      "76           77                ('LIP. 横浜 トライアル助成金',)  木原記念横浜生命科学振興財団   \n",
      "77           78              ('「横浜市被災中小企業復旧支援補助金」',)             横浜市   \n",
      "78           79               ('横浜市 小規模事業者設備投資助成金',)             横浜市   \n",
      "\n",
      "                          datahead  \n",
      "74  横浜市：2020年度 U39アーティスト・フェローシップ助成  \n",
      "75        横浜市：クリエイティブ・インクルージョン活動助成  \n",
      "76                LIP. 横浜 トライアル助成金  \n",
      "77              「横浜市被災中小企業復旧支援補助金」  \n",
      "78               横浜市 小規模事業者設備投資助成金  \n",
      "79\n",
      "79\n",
      "    data_number                                           fulltext  \\\n",
      "74           75  横浜市：2020年度 U39アーティスト・フェローシップ助成 横浜市：2020年度 U39ア...   \n",
      "75           76  横浜市：クリエイティブ・インクルージョン活動助成 横浜市：クリエイティブ・インクルージョン活...   \n",
      "76           77  LIP. 横浜 トライアル助成金 LIP. 横浜 トライアル助成金 健康・医療分野での試作品...   \n",
      "77           78  「横浜市被災中小企業復旧支援補助金」 「横浜市被災中小企業復旧支援補助金」 横浜市では、令和...   \n",
      "78           79  横浜市 小規模事業者設備投資助成金 横浜市 小規模事業者設備投資助成金 横浜市内で事業を営む...   \n",
      "\n",
      "         institute                        datahead  \n",
      "74     横浜市芸術文化振興財団  横浜市：2020年度 U39アーティスト・フェローシップ助成  \n",
      "75     横浜市芸術文化振興財団        横浜市：クリエイティブ・インクルージョン活動助成  \n",
      "76  木原記念横浜生命科学振興財団                LIP. 横浜 トライアル助成金  \n",
      "77             横浜市              「横浜市被災中小企業復旧支援補助金」  \n",
      "78             横浜市               横浜市 小規模事業者設備投資助成金  \n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "#################### 補助金・助成金データの取得 ############################\n",
    "###########################################################################\n",
    "\n",
    "hojo_yoko = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\dataleaves\\\\yokohama_hojokin_header.csv\")\n",
    "hojo_yoko[\"fulltext\"] = [f\"{dh}\" for dh in zip(hojo_yoko[\"datahead\"])]\n",
    "hojo_yoko = hojo_yoko[[\"cid\", \"fulltext\", \"institute\", \"datahead\"]].copy()\n",
    "hojo_yoko.columns = [\"data_number\", \"fulltext\", \"institute\", \"datahead\"]\n",
    "\n",
    "hojo_yoko_desc = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\dataleaves\\\\yokohama_hojokin_name.csv\")\n",
    "hojo_yoko_desc[\"fulltext\"] = [f\"{dh} {dn} {obj} {kh}\" for dh, dn, obj, kh in zip(hojo_yoko_desc[\"datahead\"], hojo_yoko_desc[\"dataname\"], hojo_yoko_desc[\"object\"], hojo_yoko_desc[\"keihi\"])]\n",
    "hojo_yoko_desc = hojo_yoko_desc[[\"cid\", \"fulltext\", \"institute\", \"datahead\"]].copy()\n",
    "hojo_yoko_desc.columns = [\"data_number\", \"fulltext\", \"institute\", \"datahead\"]\n",
    "\n",
    "sentences_hojo = []\n",
    "for add_s in hojo_yoko[\"fulltext\"]:\n",
    "    sentences_hojo.append(add_s.strip())\n",
    "sentences_hojo_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_hojo\n",
    "})\n",
    "sentences_hojo_df[\"counterpart\"] = [st.strip() for st in hojo_yoko[\"institute\"]]\n",
    "\n",
    "sentences_hojo_desc = []\n",
    "for add_s in hojo_yoko_desc[\"fulltext\"]:\n",
    "    sentences_hojo_desc.append(add_s.strip())\n",
    "sentences_hojo_desc_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_hojo_desc\n",
    "})\n",
    "sentences_hojo_desc_df[\"counterpart\"] = [st.strip() for st in hojo_yoko_desc[\"institute\"]]\n",
    "\n",
    "#Compute embeddings\n",
    "if False: \n",
    "# if True:\n",
    "    print(\"Start calc embeddings\")\n",
    "    print(datetime.datetime.now())\n",
    "    embeddings_hojo = model.encode(sentences_hojo, convert_to_tensor=True)\n",
    "    embeddings_hojo_desc = model.encode(sentences_hojo_desc, convert_to_tensor=True)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # pickle化してファイルに書き込み\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_hojo.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_hojo, f)\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_hojo_desc.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_hojo_desc, f)\n",
    "else:\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_hojo.pkl', 'rb') as f:\n",
    "        embeddings_hojo = pickle.load(f)    \n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_hojo_desc.pkl', 'rb') as f:\n",
    "        embeddings_hojo_desc = pickle.load(f)\n",
    "        \n",
    "sentences_hojo_df[\"sentence\"] = hojo_yoko[\"datahead\"]\n",
    "sentences_hojo_desc_df[\"sentence\"] = hojo_yoko_desc[\"datahead\"]\n",
    "print(len(embeddings_hojo))\n",
    "print(len(sentences_hojo_df))\n",
    "print(hojo_yoko.tail())\n",
    "print(len(embeddings_hojo_desc))\n",
    "print(len(sentences_hojo_desc_df))\n",
    "print(hojo_yoko_desc.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dae639e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n",
      "544\n",
      "     data_number                            fulltext creator\n",
      "539          540   ('   平成２９年１月 市場月報       XLS   ',)   運営調整課\n",
      "540          541  ('   平成２８年１２月 市場月報       XLS   ',)   運営調整課\n",
      "541          542  ('   平成２８年１０月 市場月報       XLS   ',)   運営調整課\n",
      "542          543   ('   平成２８年９月 市場月報       XLS   ',)   運営調整課\n",
      "543          544   ('   平成２８年８月 市場月報       XLS   ',)   運営調整課\n",
      "14208\n",
      "14208\n",
      "       data_number                                           fulltext creator\n",
      "14203        14204  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14204        14205  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14205        14206  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14206        14207  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14207        14208  ('   平成２８年８月 市場月報       XLS          食肉部\\u3000...   運営調整課\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "############ データジャケットの内容をベクトルとして保存 ####################\n",
    "###########################################################################\n",
    "\n",
    "if False:\n",
    "    dls = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\dataleaves\\\\datajacket.csv\", usecols=[\"ID\", \"title\", \"outline\", \"collecting_cost\", \"sharing_policy\", \"type\", \"variable\", \"analysis\", \"outcome\", \"anticipation\", \"comments\", \"wanted\"])\n",
    "    dls[\"fulltext\"] = [f\"{ttl}, {ol}, {v}\" for ttl, ol, v in zip(dls[\"title\"], dls[\"outline\"], dls[\"variable\"])]\n",
    "\n",
    "    sentences = []\n",
    "    for add_s in dls[\"fulltext\"]:\n",
    "        sentences.append(add_s.strip())\n",
    "\n",
    "    #Compute embeddings\n",
    "    if False:\n",
    "        print(\"Start calc embeddings\")\n",
    "        print(datetime.datetime.now())\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        # pickle化してファイルに書き込み\n",
    "        with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves.pkl', 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "    else:\n",
    "        with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves.pkl', 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "else:\n",
    "    dls_yoko = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\dataleaves\\\\yokohama_opendata_header.csv\")\n",
    "    dls_yoko[\"fulltext\"] = [f\"{dh}\" for dh in zip(dls_yoko[\"datahead\"])]\n",
    "    dls_yoko = dls_yoko[[\"cid\", \"fulltext\", \"creator\"]].copy()\n",
    "    dls_yoko.columns = [\"data_number\", \"fulltext\", \"creator\"]\n",
    "\n",
    "    dls_yoko_desc = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\dataleaves\\\\yokohama_opendata_name.csv\")\n",
    "    dls_yoko_desc[\"fulltext\"] = [f\"{dn}\" for dn in zip(dls_yoko_desc[\"dataname\"])]\n",
    "    dls_yoko_desc = dls_yoko_desc[[\"cid\", \"fulltext\", \"creator\"]].copy()\n",
    "    dls_yoko_desc.columns = [\"data_number\", \"fulltext\", \"creator\"]\n",
    "\n",
    "    sentences_yoko = []\n",
    "    for add_s in dls_yoko[\"fulltext\"]:\n",
    "        sentences_yoko.append(add_s.strip())\n",
    "    sentences_yoko_df = pd.DataFrame({\n",
    "        \"sentence\": sentences_yoko\n",
    "    })\n",
    "    sentences_yoko_df[\"counterpart\"] = [st.strip() for st in dls_yoko[\"creator\"]]\n",
    "\n",
    "    sentences_yoko_desc = []\n",
    "    for add_s in dls_yoko_desc[\"fulltext\"]:\n",
    "        sentences_yoko_desc.append(add_s.strip())\n",
    "    sentences_yoko_desc_df = pd.DataFrame({\n",
    "        \"sentence\": sentences_yoko_desc\n",
    "    })\n",
    "    sentences_yoko_desc_df[\"counterpart\"] = [st.strip() for st in dls_yoko_desc[\"creator\"]]\n",
    "\n",
    "    #Compute embeddings\n",
    "    # if False: \n",
    "    if False:\n",
    "        print(\"Start calc embeddings\")\n",
    "        print(datetime.datetime.now())\n",
    "        embeddings_yoko = model.encode(sentences_yoko, convert_to_tensor=True)\n",
    "        embeddings_yoko_desc = model.encode(sentences_yoko_desc, convert_to_tensor=True)\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        # pickle化してファイルに書き込み\n",
    "        with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_yoko.pkl', 'wb') as f:\n",
    "            pickle.dump(embeddings_yoko, f)\n",
    "        with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_yoko_desc.pkl', 'wb') as f:\n",
    "            pickle.dump(embeddings_yoko_desc, f)\n",
    "    else:\n",
    "        with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_yoko.pkl', 'rb') as f:\n",
    "            embeddings_yoko = pickle.load(f)    \n",
    "        with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\dataleaves_yoko_desc.pkl', 'rb') as f:\n",
    "            embeddings_yoko_desc = pickle.load(f)\n",
    "    print(len(embeddings_yoko))\n",
    "    print(len(sentences_yoko_df))\n",
    "    print(dls_yoko.tail())\n",
    "    print(len(embeddings_yoko_desc))\n",
    "    print(len(sentences_yoko_desc_df))\n",
    "    print(dls_yoko_desc.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "168d9d40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Vector</th>\n",
       "      <th>User</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>横浜を、地域や社会のためになる活動をやりたいと思った人が、自由に相談し、仲間を見つけ、支援者...</td>\n",
       "      <td>[0.15350482, -0.011069472, 0.36629367, -0.2243...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>1</td>\n",
       "      <td>横浜で地域活動を支援する街へ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>事業提案者の理念、チーム、スキル、事業の収益可能性、支援後の効果検証およびモニタリング等の各...</td>\n",
       "      <td>[-0.016009241, 0.96617514, 1.1718636, -0.72538...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>2</td>\n",
       "      <td>事業提案者の評価手法の開発と運用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>事業設立数、黒字事業数（含む社会的インパクト）、黒字額（含む社会的インパクト）、KPI達成進...</td>\n",
       "      <td>[0.21427147, 0.2961613, 0.11891821, -0.4338633...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>3</td>\n",
       "      <td>社会的インパクトを含む事業設立数と黒字事業の状況</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>皆さんがやりたいこと、やってみたいことを自由に投稿頂き、あれこれ議論できるようなアイデアサン...</td>\n",
       "      <td>[-0.015601786, 0.3038959, 0.58063775, 0.073553...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>4</td>\n",
       "      <td>アイデアサンドボックスの構築と運用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>市民、企業、行政が一体となり、高度な自治を実現している街</td>\n",
       "      <td>[0.26880562, 0.05276879, 0.8365536, -0.1892348...</td>\n",
       "      <td>emo</td>\n",
       "      <td>1</td>\n",
       "      <td>市民、企業、行政が一体となり、高度な自治を実現している街</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>行政と企業の対話によるオープンイノベーションの成功事例の開拓と、キャリア教育の一環としての子...</td>\n",
       "      <td>[-0.29456726, 0.026520278, 0.76736397, 0.08926...</td>\n",
       "      <td>emo</td>\n",
       "      <td>2</td>\n",
       "      <td>オープンイノベーションとキャリア教育の連携</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>行政と企業の共創による事業取り組み数・売上伸び率・行政の生産性向上指数など、小中高生の自己有...</td>\n",
       "      <td>[-0.10903569, 0.5291746, 0.030806039, -0.39204...</td>\n",
       "      <td>emo</td>\n",
       "      <td>3</td>\n",
       "      <td>横浜の行政と企業の共創による地域活性化</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ケースメソッドによる共創創発ワークショプ、子どもと横浜型地域貢献企業が一緒に参加する共創ダイアログ</td>\n",
       "      <td>[0.18595394, -0.056578014, 0.45105568, 0.06257...</td>\n",
       "      <td>emo</td>\n",
       "      <td>4</td>\n",
       "      <td>共創ワークショップとダイアログ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>誰もが自分らしさを活かし、自然や人と関わりながら楽しく優しく共生している、コンヴィヴィアルなまち</td>\n",
       "      <td>[-0.38173744, -0.06489488, 1.0201046, 0.250125...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>1</td>\n",
       "      <td>コンヴィヴィアルなまちの実現</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>情報発信（市外・海外への発信も含めて）。横浜市内各地で展開されている取り組みの価値を可視化し...</td>\n",
       "      <td>[0.23780015, -0.11628275, 0.6812856, -0.343407...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>2</td>\n",
       "      <td>横浜市の取り組みを発信し、新たな出会いと創造を促す</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>記事数、Webサイトへの訪問者数、記事によって感謝された数</td>\n",
       "      <td>[-0.22223002, 0.61757636, 1.0857458, 0.0108173...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>3</td>\n",
       "      <td>記事数、Webサイトへの訪問者数、記事によって感謝された数</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>「循環経済とケア」をテーマとするイベントや事業</td>\n",
       "      <td>[0.17047071, -0.074134335, 0.9424211, -0.33440...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>4</td>\n",
       "      <td>「循環経済とケア」をテーマとするイベントや事業</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>自然と調和しながら、そこに暮らす人が「生きがい」「やりがい」を感じながら、フォーマルなサービ...</td>\n",
       "      <td>[0.03164115, -0.19801605, 0.5024515, 0.0650409...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>1</td>\n",
       "      <td>自然と共生する持続可能な街への夢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>団地という多種多様な人々が暮らす場で、住民の声を聞き、それを行政や企業と共有しながら、実走で...</td>\n",
       "      <td>[0.36531875, 0.3715828, 0.48456734, -0.2326107...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>2</td>\n",
       "      <td>多様な住民の声を共有し、実践する仕組み作り</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>そこに暮らす人々の定量的な幸福度の調査を継続して行いデータ化する。</td>\n",
       "      <td>[0.3011839, 0.5062818, 0.581456, 0.10106163, -...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>3</td>\n",
       "      <td>地域の幸福度調査とデータ化</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>今、バラバラに動いている様々な取り組みやプロジェクトを点から面に変えていきたいと思います。</td>\n",
       "      <td>[0.17070498, -0.22292618, 0.84985346, 0.001359...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>4</td>\n",
       "      <td>取り組みを点から面に変える</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>活力ある企業活動が優れた雇用環境を市内外の人に提供し、産学官による先進的な取組があふれているまち</td>\n",
       "      <td>[-0.2317084, 0.36708373, 0.4376017, -0.5879589...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>1</td>\n",
       "      <td>活力ある企業活動と先進的な取組を通じた雇用環境の提供</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>行政と企業（と大学）が活発なオープンイノベーションに取り組める場の提供と投資の蓄積</td>\n",
       "      <td>[-0.16830441, 0.07773686, 0.56051874, -0.29683...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>2</td>\n",
       "      <td>オープンイノベーションの推進と投資の蓄積</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>オープンイノベーションの件数と企業の投資額や雇用の実績（報告ベースにはなりますが）</td>\n",
       "      <td>[0.043046802, 0.3917093, 0.26555812, -0.636787...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>3</td>\n",
       "      <td>オープンイノベーションの件数と投資額雇用の実績</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>産学官のオープンイノベーションの場は既にあるので、そこと共創コンソーシアムとの連携の場、さら...</td>\n",
       "      <td>[-0.33113685, -0.14651369, 0.5758766, -0.33886...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>4</td>\n",
       "      <td>オープンイノベーションの場の連携と実証実験の必要性</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>目の前の課題解決に忙しくするのではなく、先進的なことや創造的なことに人々の時間やお金を割くこ...</td>\n",
       "      <td>[0.10956119, -0.21474005, 0.39737132, 0.022746...</td>\n",
       "      <td>Circular Yokohama（ハーチ株式会社）</td>\n",
       "      <td>1</td>\n",
       "      <td>能動的な街の創造と先進性</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20代という立場から、自分の持っている価値観や希望、理想を積極的に発信する。</td>\n",
       "      <td>[0.408004, 0.0063840672, 0.8019861, -0.2369251...</td>\n",
       "      <td>Circular Yokohama（ハーチ株式会社）</td>\n",
       "      <td>2</td>\n",
       "      <td>20代の価値観と理想を発信する</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>市民意識調査（未来に対する希望・不安を問う設問の回答）、選挙の投票率、人口増加数、市内の世帯...</td>\n",
       "      <td>[0.56484497, -0.75186694, 0.57027036, 0.511742...</td>\n",
       "      <td>Circular Yokohama（ハーチ株式会社）</td>\n",
       "      <td>3</td>\n",
       "      <td>市民の意識と行動に関する調査データ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>良き新らしきものが良き古きものから自然に成長してくるような状況</td>\n",
       "      <td>[0.22978488, -0.40311095, 0.9669363, -0.042636...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>1</td>\n",
       "      <td>自然な成長による良き新旧の交代</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>良き新しきもの、良き古きものを探す遊歩者、それらを繋ぐ篩(ふるい)としての役割</td>\n",
       "      <td>[-0.09648992, 0.05599883, 1.4428713, 0.3177355...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>2</td>\n",
       "      <td>遊歩者の役割：新旧の良きものを繋ぐ篩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>創造的な勉強会の数、創造的なまちあるきの実施回数、継続的な対話の場の質と数</td>\n",
       "      <td>[-0.4500581, 0.2069761, 0.5258233, 0.33563566,...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>3</td>\n",
       "      <td>創造的な勉強会やまちあるきの実施と対話の質</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>いろんな局の守備範囲をまたいだ継続的な勉強会。具体的なフィールドをみなで観て考える会。市役所...</td>\n",
       "      <td>[-0.06272981, -0.268584, 0.502122, 0.37295258,...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>4</td>\n",
       "      <td>異局の連携で市役所を大学院化</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100年後の未来を見据え、子供達の礎となる利他の精神と農福スポーツ連携したまち。平時でも災害...</td>\n",
       "      <td>[-0.20543464, 0.13389595, 0.7603202, 0.2266789...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>1</td>\n",
       "      <td>未来を見据えた利他のまち</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>新しい事業を創出し、それを見える化しながら認知拡大、資金調達、事業支援を持続的に実施する。（...</td>\n",
       "      <td>[0.32653156, 0.41252556, 1.0726038, -0.9254761...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>2</td>\n",
       "      <td>新事業創出の支援と認知拡大</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>コンソーシアムの事業として、竹山団地やすすきの団地で生まれた事業が基軸となり、他の地域で事業...</td>\n",
       "      <td>[0.2564719, 0.19502932, 0.379373, -0.14425592,...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>3</td>\n",
       "      <td>コンソーシアムの事業で地域活性化</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>企業はふるさと納税獲得に向けた取り組みを団地をキーワードにタブロイド紙を作成しています。行政...</td>\n",
       "      <td>[0.012239848, 0.7328715, -0.1754152, 0.3445332...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>4</td>\n",
       "      <td>団地を活用したふるさと納税獲得の取り組み</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>自立型経済都市として自立し、愛を持って人や物とか関わりを持てる街</td>\n",
       "      <td>[0.11597422, 0.08454424, 0.66753817, 0.1093635...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>1</td>\n",
       "      <td>自立型経済都市として愛を持つ街</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>共創案件のブリッジ人材</td>\n",
       "      <td>[0.17846952, 0.2612569, 0.94532585, 0.12821315...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>2</td>\n",
       "      <td>共創案件のブリッジ人材</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>自立事業としての案件成功数、経済的（雇用率等含む）な結果</td>\n",
       "      <td>[-0.092436336, 0.7118591, 0.34000847, -0.45233...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>3</td>\n",
       "      <td>自立事業としての案件成功数、経済的（雇用率等含む）な結果</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ポケモンや、ガンダム等のインパクトのある事業（関係人口の増加含む）</td>\n",
       "      <td>[-0.023184938, -0.62203735, 0.5721721, -0.5575...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>4</td>\n",
       "      <td>ポケモンやガンダムなどの人気事業</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>18区それぞれに特徴があるので、中区、西区の中心だけでなく、その彩り豊かな特徴を出し合えるまち</td>\n",
       "      <td>[0.7407466, -0.31217068, 0.7212526, 0.14771113...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>1</td>\n",
       "      <td>神戸市の18区はそれぞれ特色があり、多様な魅力を持つまち</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>それぞれに特徴を活かしあえる情報発信基地</td>\n",
       "      <td>[0.21762566, -0.029377738, 1.1780556, 0.329879...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>2</td>\n",
       "      <td>それぞれに特徴を活かしあえる情報発信基地</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Well-being指標で図る。市民総幸福量の創設など。</td>\n",
       "      <td>[0.47751284, 0.21097943, 0.76513785, -0.016607...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>3</td>\n",
       "      <td>Well-being指標で図る。市民総幸福量の創設など。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>人口減少社会でも、明るく仲良く、行ってみたい、住んでみたいまちとなるためにも、みんながまちづ...</td>\n",
       "      <td>[0.15831795, -0.12689795, 0.73806137, 0.061832...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>4</td>\n",
       "      <td>人口減少社会のまちづくりに参加しよう</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街</td>\n",
       "      <td>[0.02384767, 0.10383327, 0.49484348, -0.550355...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>1</td>\n",
       "      <td>企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>自治体ニーズと企業シーズの触媒役</td>\n",
       "      <td>[0.2534485, 0.16347377, 0.8360413, -0.2745385,...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>2</td>\n",
       "      <td>自治体ニーズと企業シーズの触媒役</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>実証実験数、寄付件数、寄付金額</td>\n",
       "      <td>[0.28331488, -0.016000971, -0.05473207, -0.022...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>3</td>\n",
       "      <td>実証実験数、寄付件数、寄付金額</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>触媒役をできる人財育成。具体的には、中小企業診断士資格保有者で企業内にいる（独立していない）...</td>\n",
       "      <td>[0.23426801, 0.52402204, 0.7462301, -0.2943097...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>4</td>\n",
       "      <td>中小企業診断士の実務経験を積むための研修プログラム</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>エコロジーとテクノロジーとの共生、人と人のコミュニケーションリテラシーの向上</td>\n",
       "      <td>[-0.060757987, -0.065611996, 0.7962137, -0.241...</td>\n",
       "      <td>いよいよ</td>\n",
       "      <td>1</td>\n",
       "      <td>エコとテクノの共生と人々のコミュニケーション向上</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>リビングラボ全体のボトムアップとリブランディング、CSRのように一般化させるために地域貢献企...</td>\n",
       "      <td>[0.05243929, 0.3335145, 1.0886495, -0.23916566...</td>\n",
       "      <td>いよいよ</td>\n",
       "      <td>2</td>\n",
       "      <td>地域貢献企業認定制度の創設</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>スキームが構築できた時</td>\n",
       "      <td>[0.11055018, -0.41170222, 1.3172058, 0.0579606...</td>\n",
       "      <td>いよいよ</td>\n",
       "      <td>3</td>\n",
       "      <td>スキームが構築できた時</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   横浜を、地域や社会のためになる活動をやりたいと思った人が、自由に相談し、仲間を見つけ、支援者...   \n",
       "1   事業提案者の理念、チーム、スキル、事業の収益可能性、支援後の効果検証およびモニタリング等の各...   \n",
       "2   事業設立数、黒字事業数（含む社会的インパクト）、黒字額（含む社会的インパクト）、KPI達成進...   \n",
       "3   皆さんがやりたいこと、やってみたいことを自由に投稿頂き、あれこれ議論できるようなアイデアサン...   \n",
       "4                        市民、企業、行政が一体となり、高度な自治を実現している街   \n",
       "5   行政と企業の対話によるオープンイノベーションの成功事例の開拓と、キャリア教育の一環としての子...   \n",
       "6   行政と企業の共創による事業取り組み数・売上伸び率・行政の生産性向上指数など、小中高生の自己有...   \n",
       "7   ケースメソッドによる共創創発ワークショプ、子どもと横浜型地域貢献企業が一緒に参加する共創ダイアログ   \n",
       "8    誰もが自分らしさを活かし、自然や人と関わりながら楽しく優しく共生している、コンヴィヴィアルなまち   \n",
       "9   情報発信（市外・海外への発信も含めて）。横浜市内各地で展開されている取り組みの価値を可視化し...   \n",
       "10                      記事数、Webサイトへの訪問者数、記事によって感謝された数   \n",
       "11                            「循環経済とケア」をテーマとするイベントや事業   \n",
       "12  自然と調和しながら、そこに暮らす人が「生きがい」「やりがい」を感じながら、フォーマルなサービ...   \n",
       "13  団地という多種多様な人々が暮らす場で、住民の声を聞き、それを行政や企業と共有しながら、実走で...   \n",
       "14                  そこに暮らす人々の定量的な幸福度の調査を継続して行いデータ化する。   \n",
       "15      今、バラバラに動いている様々な取り組みやプロジェクトを点から面に変えていきたいと思います。   \n",
       "16   活力ある企業活動が優れた雇用環境を市内外の人に提供し、産学官による先進的な取組があふれているまち   \n",
       "17          行政と企業（と大学）が活発なオープンイノベーションに取り組める場の提供と投資の蓄積   \n",
       "18          オープンイノベーションの件数と企業の投資額や雇用の実績（報告ベースにはなりますが）   \n",
       "19  産学官のオープンイノベーションの場は既にあるので、そこと共創コンソーシアムとの連携の場、さら...   \n",
       "20  目の前の課題解決に忙しくするのではなく、先進的なことや創造的なことに人々の時間やお金を割くこ...   \n",
       "21             20代という立場から、自分の持っている価値観や希望、理想を積極的に発信する。   \n",
       "22  市民意識調査（未来に対する希望・不安を問う設問の回答）、選挙の投票率、人口増加数、市内の世帯...   \n",
       "23                    良き新らしきものが良き古きものから自然に成長してくるような状況   \n",
       "24            良き新しきもの、良き古きものを探す遊歩者、それらを繋ぐ篩(ふるい)としての役割   \n",
       "25              創造的な勉強会の数、創造的なまちあるきの実施回数、継続的な対話の場の質と数   \n",
       "26  いろんな局の守備範囲をまたいだ継続的な勉強会。具体的なフィールドをみなで観て考える会。市役所...   \n",
       "27  100年後の未来を見据え、子供達の礎となる利他の精神と農福スポーツ連携したまち。平時でも災害...   \n",
       "28  新しい事業を創出し、それを見える化しながら認知拡大、資金調達、事業支援を持続的に実施する。（...   \n",
       "29  コンソーシアムの事業として、竹山団地やすすきの団地で生まれた事業が基軸となり、他の地域で事業...   \n",
       "30  企業はふるさと納税獲得に向けた取り組みを団地をキーワードにタブロイド紙を作成しています。行政...   \n",
       "31                   自立型経済都市として自立し、愛を持って人や物とか関わりを持てる街   \n",
       "32                                        共創案件のブリッジ人材   \n",
       "33                       自立事業としての案件成功数、経済的（雇用率等含む）な結果   \n",
       "34                  ポケモンや、ガンダム等のインパクトのある事業（関係人口の増加含む）   \n",
       "35    18区それぞれに特徴があるので、中区、西区の中心だけでなく、その彩り豊かな特徴を出し合えるまち   \n",
       "36                               それぞれに特徴を活かしあえる情報発信基地   \n",
       "37                       Well-being指標で図る。市民総幸福量の創設など。   \n",
       "38  人口減少社会でも、明るく仲良く、行ってみたい、住んでみたいまちとなるためにも、みんながまちづ...   \n",
       "39                       企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街   \n",
       "40                                   自治体ニーズと企業シーズの触媒役   \n",
       "41                                    実証実験数、寄付件数、寄付金額   \n",
       "42  触媒役をできる人財育成。具体的には、中小企業診断士資格保有者で企業内にいる（独立していない）...   \n",
       "43             エコロジーとテクノロジーとの共生、人と人のコミュニケーションリテラシーの向上   \n",
       "44  リビングラボ全体のボトムアップとリブランディング、CSRのように一般化させるために地域貢献企...   \n",
       "45                                        スキームが構築できた時   \n",
       "\n",
       "                                               Vector  \\\n",
       "0   [0.15350482, -0.011069472, 0.36629367, -0.2243...   \n",
       "1   [-0.016009241, 0.96617514, 1.1718636, -0.72538...   \n",
       "2   [0.21427147, 0.2961613, 0.11891821, -0.4338633...   \n",
       "3   [-0.015601786, 0.3038959, 0.58063775, 0.073553...   \n",
       "4   [0.26880562, 0.05276879, 0.8365536, -0.1892348...   \n",
       "5   [-0.29456726, 0.026520278, 0.76736397, 0.08926...   \n",
       "6   [-0.10903569, 0.5291746, 0.030806039, -0.39204...   \n",
       "7   [0.18595394, -0.056578014, 0.45105568, 0.06257...   \n",
       "8   [-0.38173744, -0.06489488, 1.0201046, 0.250125...   \n",
       "9   [0.23780015, -0.11628275, 0.6812856, -0.343407...   \n",
       "10  [-0.22223002, 0.61757636, 1.0857458, 0.0108173...   \n",
       "11  [0.17047071, -0.074134335, 0.9424211, -0.33440...   \n",
       "12  [0.03164115, -0.19801605, 0.5024515, 0.0650409...   \n",
       "13  [0.36531875, 0.3715828, 0.48456734, -0.2326107...   \n",
       "14  [0.3011839, 0.5062818, 0.581456, 0.10106163, -...   \n",
       "15  [0.17070498, -0.22292618, 0.84985346, 0.001359...   \n",
       "16  [-0.2317084, 0.36708373, 0.4376017, -0.5879589...   \n",
       "17  [-0.16830441, 0.07773686, 0.56051874, -0.29683...   \n",
       "18  [0.043046802, 0.3917093, 0.26555812, -0.636787...   \n",
       "19  [-0.33113685, -0.14651369, 0.5758766, -0.33886...   \n",
       "20  [0.10956119, -0.21474005, 0.39737132, 0.022746...   \n",
       "21  [0.408004, 0.0063840672, 0.8019861, -0.2369251...   \n",
       "22  [0.56484497, -0.75186694, 0.57027036, 0.511742...   \n",
       "23  [0.22978488, -0.40311095, 0.9669363, -0.042636...   \n",
       "24  [-0.09648992, 0.05599883, 1.4428713, 0.3177355...   \n",
       "25  [-0.4500581, 0.2069761, 0.5258233, 0.33563566,...   \n",
       "26  [-0.06272981, -0.268584, 0.502122, 0.37295258,...   \n",
       "27  [-0.20543464, 0.13389595, 0.7603202, 0.2266789...   \n",
       "28  [0.32653156, 0.41252556, 1.0726038, -0.9254761...   \n",
       "29  [0.2564719, 0.19502932, 0.379373, -0.14425592,...   \n",
       "30  [0.012239848, 0.7328715, -0.1754152, 0.3445332...   \n",
       "31  [0.11597422, 0.08454424, 0.66753817, 0.1093635...   \n",
       "32  [0.17846952, 0.2612569, 0.94532585, 0.12821315...   \n",
       "33  [-0.092436336, 0.7118591, 0.34000847, -0.45233...   \n",
       "34  [-0.023184938, -0.62203735, 0.5721721, -0.5575...   \n",
       "35  [0.7407466, -0.31217068, 0.7212526, 0.14771113...   \n",
       "36  [0.21762566, -0.029377738, 1.1780556, 0.329879...   \n",
       "37  [0.47751284, 0.21097943, 0.76513785, -0.016607...   \n",
       "38  [0.15831795, -0.12689795, 0.73806137, 0.061832...   \n",
       "39  [0.02384767, 0.10383327, 0.49484348, -0.550355...   \n",
       "40  [0.2534485, 0.16347377, 0.8360413, -0.2745385,...   \n",
       "41  [0.28331488, -0.016000971, -0.05473207, -0.022...   \n",
       "42  [0.23426801, 0.52402204, 0.7462301, -0.2943097...   \n",
       "43  [-0.060757987, -0.065611996, 0.7962137, -0.241...   \n",
       "44  [0.05243929, 0.3335145, 1.0886495, -0.23916566...   \n",
       "45  [0.11055018, -0.41170222, 1.3172058, 0.0579606...   \n",
       "\n",
       "                          User  Answer                        Keyword  \n",
       "0                         TA前川       1                 横浜で地域活動を支援する街へ  \n",
       "1                         TA前川       2               事業提案者の評価手法の開発と運用  \n",
       "2                         TA前川       3       社会的インパクトを含む事業設立数と黒字事業の状況  \n",
       "3                         TA前川       4              アイデアサンドボックスの構築と運用  \n",
       "4                          emo       1   市民、企業、行政が一体となり、高度な自治を実現している街  \n",
       "5                          emo       2          オープンイノベーションとキャリア教育の連携  \n",
       "6                          emo       3            横浜の行政と企業の共創による地域活性化  \n",
       "7                          emo       4                共創ワークショップとダイアログ  \n",
       "8                           Yu       1                 コンヴィヴィアルなまちの実現  \n",
       "9                           Yu       2      横浜市の取り組みを発信し、新たな出会いと創造を促す  \n",
       "10                          Yu       3  記事数、Webサイトへの訪問者数、記事によって感謝された数  \n",
       "11                          Yu       4        「循環経済とケア」をテーマとするイベントや事業  \n",
       "12         一般社団法人団地暮らしの共創　小柴健一       1               自然と共生する持続可能な街への夢  \n",
       "13         一般社団法人団地暮らしの共創　小柴健一       2          多様な住民の声を共有し、実践する仕組み作り  \n",
       "14         一般社団法人団地暮らしの共創　小柴健一       3                  地域の幸福度調査とデータ化  \n",
       "15         一般社団法人団地暮らしの共創　小柴健一       4                  取り組みを点から面に変える  \n",
       "16                       きんちゃん       1     活力ある企業活動と先進的な取組を通じた雇用環境の提供  \n",
       "17                       きんちゃん       2           オープンイノベーションの推進と投資の蓄積  \n",
       "18                       きんちゃん       3        オープンイノベーションの件数と投資額雇用の実績  \n",
       "19                       きんちゃん       4      オープンイノベーションの場の連携と実証実験の必要性  \n",
       "20  Circular Yokohama（ハーチ株式会社）       1                   能動的な街の創造と先進性  \n",
       "21  Circular Yokohama（ハーチ株式会社）       2                20代の価値観と理想を発信する  \n",
       "22  Circular Yokohama（ハーチ株式会社）       3              市民の意識と行動に関する調査データ  \n",
       "23                        藤原徹平       1                自然な成長による良き新旧の交代  \n",
       "24                        藤原徹平       2             遊歩者の役割：新旧の良きものを繋ぐ篩  \n",
       "25                        藤原徹平       3          創造的な勉強会やまちあるきの実施と対話の質  \n",
       "26                        藤原徹平       4                 異局の連携で市役所を大学院化  \n",
       "27      NPO法人KUSC・神奈川大学サッカー部藤森       1                   未来を見据えた利他のまち  \n",
       "28      NPO法人KUSC・神奈川大学サッカー部藤森       2                  新事業創出の支援と認知拡大  \n",
       "29      NPO法人KUSC・神奈川大学サッカー部藤森       3               コンソーシアムの事業で地域活性化  \n",
       "30      NPO法人KUSC・神奈川大学サッカー部藤森       4           団地を活用したふるさと納税獲得の取り組み  \n",
       "31            コミュニティデザイン・ラボ　町山       1                自立型経済都市として愛を持つ街  \n",
       "32            コミュニティデザイン・ラボ　町山       2                    共創案件のブリッジ人材  \n",
       "33            コミュニティデザイン・ラボ　町山       3   自立事業としての案件成功数、経済的（雇用率等含む）な結果  \n",
       "34            コミュニティデザイン・ラボ　町山       4               ポケモンやガンダムなどの人気事業  \n",
       "35                   とつかリビングラボ       1   神戸市の18区はそれぞれ特色があり、多様な魅力を持つまち  \n",
       "36                   とつかリビングラボ       2           それぞれに特徴を活かしあえる情報発信基地  \n",
       "37                   とつかリビングラボ       3   Well-being指標で図る。市民総幸福量の創設など。  \n",
       "38                   とつかリビングラボ       4             人口減少社会のまちづくりに参加しよう  \n",
       "39                        ミッキー       1   企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街  \n",
       "40                        ミッキー       2               自治体ニーズと企業シーズの触媒役  \n",
       "41                        ミッキー       3                実証実験数、寄付件数、寄付金額  \n",
       "42                        ミッキー       4      中小企業診断士の実務経験を積むための研修プログラム  \n",
       "43                        いよいよ       1       エコとテクノの共生と人々のコミュニケーション向上  \n",
       "44                        いよいよ       2                  地域貢献企業認定制度の創設  \n",
       "45                        いよいよ       3                    スキームが構築できた時  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################################################################\n",
    "##########################     FC登録     #################################\n",
    "###########################################################################\n",
    "\n",
    "qst_goal = f\"\"\"\n",
    "    みなさんが創りたい未来の横浜 、果たしたい役割、 コンソーシアムとしてやりたいことについてお聞かせください。\n",
    "\"\"\"\n",
    "\n",
    "qst_list = [\n",
    "    \"Qst,（１）あなたが創りたい未来の横浜とは、どのようなものですか？,自由コメント\", \n",
    "    \"Qst,（２）創りたい未来の実現のため、あなたはコンソーシアム内でどのような役割を果たしたいですか？,自由コメント\", \n",
    "    \"Qst,（３）その役割が果たせたかどうかを、どのような指標で測ることができると思いますか？,自由コメント\", \n",
    "    \"Qst,（４）今後に向け、こういうことが出来たらいいな、やってみたいな、という事業あるいは会議体等があれば自由にご回答ください。,自由コメント\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "if False:\n",
    "    print(\"Start calc extraction\")\n",
    "    print(datetime.datetime.now())\n",
    "    opt_df = pd.read_csv(\"D:\\\\code\\\\FCDL\\\\みんなで創りたい未来の横浜（回答） - フォームの回答 1.csv\")\n",
    "    opt_df.columns = [\"tstamp\", \"mail\", \"ans1\", \"ans2\", \"ans3\", \"ans4\", \"email\"]\n",
    "    ans_cols_df = pd.DataFrame({\"Col\": opt_df.columns})\n",
    "    ans_cols_df[\"flag\"] = [1 if (ch[:3] == \"ans\") & (len(ch) < 10) else 0 for ch in ans_cols_df[\"Col\"]]\n",
    "    ans_cols_df = ans_cols_df[ans_cols_df[\"flag\"] == 1].reset_index(drop=True)\n",
    "    ans_max = len(ans_cols_df)\n",
    "\n",
    "    fc_sentence_list = []\n",
    "    fc_vector_list = []\n",
    "    fc_user_list = []\n",
    "    fc_ans_list =[]\n",
    "    fc_keyword_list = []\n",
    "    for user_i in range(len(opt_df)):\n",
    "        print(f\"Now calculating {str(user_i+1)} of {str(len(opt_df))}'s data...\")\n",
    "        user = opt_df[\"email\"][user_i]\n",
    "        for ans_i in range(ans_max):\n",
    "            fc_sentence = opt_df[f\"ans{str(ans_i+1)}\"][user_i]\n",
    "            if type(fc_sentence) == float:\n",
    "                fc_sentence = \"\"\n",
    "            else:\n",
    "                fc_sentence = fc_sentence.strip()\n",
    "                \n",
    "            if len(fc_sentence) >= 3:\n",
    "                fc_sentence_list.append(fc_sentence)\n",
    "                fc_vector_list.append(get_sentence_vector(fc_sentence))\n",
    "                fc_user_list.append(user)\n",
    "                fc_ans_list.append(ans_i+1)\n",
    "                    \n",
    "                if len(fc_sentence) < 30:\n",
    "                    fc_keyword_list.append(fc_sentence)\n",
    "                else:\n",
    "                    fc_keyword = get_results_for_one_text(fc_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_KADAI_CORP)\n",
    "                    fc_keyword_list.append(fc_keyword[0].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\").replace(\"「\", \"\").replace(\"」\", \"\"))\n",
    "\n",
    "    fc_all = pd.DataFrame({\n",
    "        'Sentence': fc_sentence_list, \n",
    "        'Vector': fc_vector_list, \n",
    "        'User': fc_user_list, \n",
    "        'Answer': fc_ans_list, \n",
    "        'Keyword': fc_keyword_list\n",
    "    })\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # pickle化してファイルに書き込み\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\extraction.pkl', 'wb') as f:\n",
    "        pickle.dump(fc_all, f)\n",
    "else:\n",
    "    with open('D:\\\\code\\\\FCDL\\\\dataleaves\\\\extraction.pkl', 'rb') as f:\n",
    "        fc_all = pickle.load(f)\n",
    "\n",
    "user_all = fc_all[\"User\"].unique()\n",
    "answer_all = fc_all[\"Answer\"].unique()\n",
    "fc_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652dba8",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Similarity Calculation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "08ed5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Clustering and network creation ##################################\n",
    "def clustering_and_create_network(fc_df, embdata, embkadai, embcorp, embhojo, sentencedata, sentencekadai, sentencecorp, sentencehojo, seg, num_clusters, network_num):\n",
    "    #k-means法でDLをクラスタリング\n",
    "    clustering = clustering_Kmeans(fc_df, num_clusters, \"normalization\")\n",
    "    fc_emb_df = fc_df[[\"Sentence\", \"Cluster\"]].groupby([\"Cluster\"])[\"Sentence\"].apply('。'.join).reset_index()\n",
    "\n",
    "    emb_list = []\n",
    "    for i in range(len(clustering.cluster_centers_)):\n",
    "        emb_list.append(model.encode(fc_emb_df[\"Sentence\"][i], convert_to_tensor=True))\n",
    "\n",
    "    # 入力文と検索対象文のベクトル表現の類似度を計算\n",
    "    tgt_cluster_list = []\n",
    "    dl_sentence_list = []\n",
    "    dl_num_list = []\n",
    "    dl_corp_list = []\n",
    "    dl_dtype_list = []\n",
    "    for emb_i in range(len(emb_list)):\n",
    "        embedding = emb_list[emb_i]\n",
    "        \n",
    "        # data\n",
    "        scores = util.pytorch_cos_sim(embedding, embdata)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(5):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencedata[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLData,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLData\")\n",
    "            dl_corp_list.append(sentencedata[\"counterpart\"][predicted_idx])\n",
    "\n",
    "        # kadai\n",
    "        scores = util.pytorch_cos_sim(embedding, embkadai)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(2):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencekadai[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLKadai,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLKadai\")\n",
    "            dl_corp_list.append(sentencekadai[\"counterpart\"][predicted_idx])\n",
    "            \n",
    "        # corp\n",
    "        scores = util.pytorch_cos_sim(embedding, embcorp)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(3):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencecorp[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLCorp,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLCorp\")\n",
    "            dl_corp_list.append(sentencecorp[\"counterpart\"][predicted_idx])\n",
    "            \n",
    "        # hojo\n",
    "        scores = util.pytorch_cos_sim(embedding, embhojo)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(2):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencehojo[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLHojo,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLHojo\")\n",
    "            dl_corp_list.append(sentencehojo[\"counterpart\"][predicted_idx])\n",
    "            \n",
    "        # all answer\n",
    "        fc_flt = fc_df[fc_df[\"Cluster\"] == emb_i].reset_index(drop=True)\n",
    "        for fc_i in range(len(fc_flt)):\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(fc_flt[\"Keyword\"][fc_i])\n",
    "            dl_num_list.append(f\"UserAns,{str(emb_i)}\")\n",
    "            dl_dtype_list.append(\"UserAns\")\n",
    "            dl_corp_list.append(fc_flt[\"User\"][fc_i])\n",
    "            \n",
    "            \n",
    "    dl_df = pd.DataFrame({\n",
    "        'Sentence': dl_sentence_list, \n",
    "        'User': dl_num_list, \n",
    "        'Cluster': tgt_cluster_list, \n",
    "        'Corp': dl_corp_list, \n",
    "        'DataType': dl_dtype_list\n",
    "    })\n",
    "    dl_emb_df = dl_df.copy()\n",
    "\n",
    "    # 各クラスタからKeyPhraseを取得\n",
    "    fc_keyword_all = []\n",
    "    fc_users = []\n",
    "    fc_cluster_list = []\n",
    "    fc_vector_list = []\n",
    "    fc_corp_list = []\n",
    "    for i in range(len(fc_emb_df)):\n",
    "        fc_sentence = fc_emb_df[\"Sentence\"][i]\n",
    "        fc_keyword_list = get_results_for_one_text(fc_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_FC)\n",
    "        for kw_i in range(len(fc_keyword_list)):\n",
    "            kw = fc_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "            \n",
    "            # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "            if (len(kw) >= 3):\n",
    "                fc_keyword_all.append(kw)\n",
    "                fc_users.append(\"FC\")\n",
    "                fc_cluster_list.append(i)\n",
    "                fc_vector_list.append(get_sentence_vector(kw))\n",
    "                fc_corp_list.append(\"\")\n",
    "\n",
    "        flt_dl = dl_emb_df[dl_emb_df[\"Cluster\"] == i].reset_index(drop=True)\n",
    "        for dl_i in range(len(flt_dl)):\n",
    "            dl_sentence = flt_dl[\"Sentence\"][dl_i]\n",
    "            dl_user = flt_dl[\"DataType\"][dl_i]\n",
    "            dl_corp = flt_dl[\"Corp\"][dl_i]\n",
    "            if dl_user == \"DLData\":\n",
    "                kw = dl_sentence.strip().replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                if (len(kw) >= 3):\n",
    "                    fc_keyword_all.append(kw)\n",
    "                    fc_users.append(dl_user)\n",
    "                    fc_cluster_list.append(i)\n",
    "                    fc_vector_list.append(get_sentence_vector(kw))\n",
    "                    fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"DLKadai\":\n",
    "                dl_keyword_list = get_results_for_one_text(dl_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_KADAI_CORP)\n",
    "                for kw_i in range(len(dl_keyword_list)):\n",
    "                    kw = dl_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                    # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                    if (len(kw) >= 3):\n",
    "                        fc_keyword_all.append(kw)\n",
    "                        fc_users.append(dl_user)\n",
    "                        fc_cluster_list.append(i)\n",
    "                        fc_vector_list.append(get_sentence_vector(kw))\n",
    "                        fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"DLCorp\":\n",
    "                dl_keyword_list = get_results_for_one_text(dl_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_KADAI_CORP)\n",
    "                for kw_i in range(len(dl_keyword_list)):\n",
    "                    kw = dl_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                    # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                    if (len(kw) >= 3):\n",
    "                        fc_keyword_all.append(kw)\n",
    "                        fc_users.append(dl_user)\n",
    "                        fc_cluster_list.append(i)\n",
    "                        fc_vector_list.append(get_sentence_vector(kw))\n",
    "                        fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"DLHojo\":\n",
    "                kw = dl_sentence.strip().replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                if (len(kw) >= 3):\n",
    "                    fc_keyword_all.append(kw)\n",
    "                    fc_users.append(dl_user)\n",
    "                    fc_cluster_list.append(i)\n",
    "                    fc_vector_list.append(get_sentence_vector(kw))\n",
    "                    fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"UserAns\":\n",
    "                kw = dl_sentence.strip().replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                if (len(kw) >= 3):\n",
    "                    fc_keyword_all.append(kw)\n",
    "                    fc_users.append(dl_user)\n",
    "                    fc_cluster_list.append(i)\n",
    "                    fc_vector_list.append(get_sentence_vector(kw))\n",
    "                    fc_corp_list.append(dl_corp)\n",
    "            else:\n",
    "                dl_keyword_list = get_results_for_one_text(dl_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_OTR)\n",
    "                for kw_i in range(len(dl_keyword_list)):\n",
    "                    kw = dl_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                    # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                    if (len(kw) >= 3):\n",
    "                        fc_keyword_all.append(kw)\n",
    "                        fc_users.append(dl_user)\n",
    "                        fc_cluster_list.append(i)\n",
    "                        fc_vector_list.append(get_sentence_vector(kw))\n",
    "                        fc_corp_list.append(dl_corp)\n",
    "\n",
    "    fc_df_base = pd.DataFrame({\n",
    "        'Keyword': fc_keyword_all, \n",
    "        'User': fc_users, \n",
    "        'Cluster': fc_cluster_list, \n",
    "        'Vector': fc_vector_list, \n",
    "        \"Corp\": fc_corp_list\n",
    "    })\n",
    "\n",
    "    vectors = []\n",
    "    for i in range(len(fc_df_base)):\n",
    "        vectors.append(np.array(fc_df_base[\"Vector\"][i]))\n",
    "    norm_vectors = normalize(vectors)\n",
    "    fc_df_base[\"NormVector\"] = [nv for nv in norm_vectors]\n",
    "    fc_df_base[\"UserCluster\"] = [f\"{usr}{clst}\" for usr, clst in zip(fc_df_base[\"User\"], fc_df_base[\"Cluster\"])]\n",
    "    \n",
    "    # FC単体情報\n",
    "    fc_df_onlyfc = fc_df_base[fc_df_base[\"User\"] == \"FC\"].reset_index(drop=True).copy()\n",
    "    key_pair_list_onlyfc = list(itertools.combinations(fc_df_onlyfc[\"Keyword\"], 2))\n",
    "    key1_list = []\n",
    "    key2_list = []\n",
    "    sim_list = []\n",
    "    for i in range(len(key_pair_list_onlyfc)):\n",
    "        key1 = key_pair_list_onlyfc[i][0]\n",
    "        key2 = key_pair_list_onlyfc[i][1]\n",
    "        if fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key1].reset_index(drop=True)[\"UserCluster\"][0] != fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key2].reset_index(drop=True)[\"UserCluster\"][0]:\n",
    "            vec1 = fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key1].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            vec2 = fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key2].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            sim_key1_key2 = get_similarity(vec1, vec2)\n",
    "            key1_list.append(key1)\n",
    "            key2_list.append(key2)\n",
    "            sim_list.append(sim_key1_key2)\n",
    "\n",
    "    key_pair_df_onlyfc = pd.DataFrame({\n",
    "        'key1': key1_list, \n",
    "        'key2': key2_list, \n",
    "        'similarity': sim_list\n",
    "    })\n",
    "    print(len(key_pair_df_onlyfc))\n",
    "    \n",
    "    # FC&DL両方\n",
    "    fc_df_all = fc_df_base.copy()\n",
    "    key_pair_list_all = list(itertools.combinations(fc_df_all[\"Keyword\"], 2))\n",
    "    key1_list = []\n",
    "    key2_list = []\n",
    "    sim_list = []\n",
    "    for i in range(len(key_pair_list_all)):\n",
    "        key1 = key_pair_list_all[i][0]\n",
    "        key2 = key_pair_list_all[i][1]\n",
    "        if fc_df_all[fc_df_all[\"Keyword\"] == key1].reset_index(drop=True)[\"UserCluster\"][0] != fc_df_all[fc_df_all[\"Keyword\"] == key2].reset_index(drop=True)[\"UserCluster\"][0]:\n",
    "            vec1 = fc_df_all[fc_df_all[\"Keyword\"] == key1].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            vec2 = fc_df_all[fc_df_all[\"Keyword\"] == key2].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            sim_key1_key2 = get_similarity(vec1, vec2)\n",
    "            key1_list.append(key1)\n",
    "            key2_list.append(key2)\n",
    "            sim_list.append(sim_key1_key2)\n",
    "\n",
    "    key_pair_df_all = pd.DataFrame({\n",
    "        'key1': key1_list, \n",
    "        'key2': key2_list, \n",
    "        'similarity': sim_list\n",
    "    })\n",
    "    print(len(key_pair_df_all))\n",
    "\n",
    "    # HTMLを作成\n",
    "    # create_fcdl_network(fc_df_onlyfc, key_pair_df_onlyfc, False, seg, \"onlyfc\", fc_emb_df, network_num, fc_df)\n",
    "    create_fcdl_network(fc_df_all, key_pair_df_all, False, seg, \"fcdl\", fc_emb_df, network_num, fc_df)\n",
    "    # create_fcdl_network(fc_df_onlyfc, key_pair_df_onlyfc, True, seg, \"onlyfc_img\", fc_emb_df, network_num, fc_df)\n",
    "    # create_fcdl_network(fc_df_all, key_pair_df_all, True, seg, \"fcdl_img\", fc_emb_df, network_num, fc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9716a09",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Visualization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5739471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Visualization ##################################\n",
    "def create_fcdl_network(fc_df, key_pair_df, image_flag, fname, tp, emb_df, network_num, orig_df):\n",
    "    flag_master = pd.DataFrame({\n",
    "        \"User\": [\"FC\", \"DLData\", \"DLKadai\", \"DLCorp\", \"DLHojo\", \"UserAns\"], \n",
    "        \"Flag\": [1, 2, 3, 4, 5, 6], \n",
    "        \"NodeColor\": [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\", \"#ffd92f\"], \n",
    "        \"EdgeColor\": [\"#b3e2cd\", \"#fdcdac\", \"#cbd5e8\", \"#f4cae4\", \"#e6f5c9\", \"#fff2ae\"]\n",
    "    })\n",
    "    \n",
    "    connect_df1 = fc_df[[\"Keyword\", \"UserCluster\"]]\n",
    "    connect_df1.columns = [\"key1\", \"key2\"]\n",
    "    connect_df2 = key_pair_df.sort_values([\"similarity\"], ascending=False).head(network_num).reset_index(drop=True)[[\"key1\", \"key2\"]]\n",
    "    \n",
    "    if tp == \"fcdl\":\n",
    "        fcdl_k1 = []\n",
    "        fcdl_k2 = []\n",
    "        for dttp in [\"DLData\", \"DLKadai\", \"DLCorp\", \"DLHojo\", \"UserAns\"]:\n",
    "            for i in range(len(emb_df)):\n",
    "                fcdl_k1.append(f\"FC{str(i)}\")\n",
    "                fcdl_k2.append(f\"{dttp}{str(i)}\")\n",
    "        connect_df3 = pd.DataFrame({\n",
    "            'key1': fcdl_k1, \n",
    "            'key2': fcdl_k2\n",
    "        })\n",
    "        \n",
    "        connect_df4 = fc_df[[\"Keyword\", \"Corp\"]].copy()\n",
    "        connect_df4 = connect_df4[connect_df4[\"Keyword\"] != \"\"]\n",
    "        connect_df4 = connect_df4[connect_df4[\"Corp\"] != \"\"]\n",
    "        connect_df4.columns = [\"key1\", \"key2\"]\n",
    "        connect_df5 = orig_df[[\"Keyword\", \"User\"]].copy()\n",
    "        connect_df5 = connect_df5[connect_df5[\"Keyword\"] != \"\"]\n",
    "        connect_df5 = connect_df5[connect_df5[\"User\"] != \"\"]\n",
    "        connect_df5.columns = [\"key1\", \"key2\"]\n",
    "        # connect_df6 = orig_df[orig_df[\"Answer\"] == 1].reset_index(drop=True).copy()\n",
    "        # connect_df6 = orig_df.copy()\n",
    "        # connect_df6 = connect_df6[[\"Keyword\", \"Cluster\"]].copy()\n",
    "        # connect_df6[\"Cluster\"] = [f\"FC{str(clst)}\" for clst in connect_df6[\"Cluster\"]]\n",
    "        # connect_df6 = connect_df6[connect_df6[\"Keyword\"] != \"\"]\n",
    "        # connect_df6 = connect_df6[connect_df6[\"Cluster\"] != \"\"]\n",
    "        # connect_df6.columns = [\"key1\", \"key2\"]\n",
    "        \n",
    "        \n",
    "        connect_df = pd.concat([connect_df1, connect_df2, connect_df3, connect_df4, connect_df5]).dropna().drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        connect_df = pd.concat([connect_df1, connect_df2]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    onlyfc_gp = pd.merge(fc_df[[\"Keyword\", \"User\"]], flag_master[[\"User\", \"Flag\"]], on=\"User\", how=\"left\")[[\"Keyword\", \"Flag\"]]\n",
    "    onlyfc_gp.columns = [\"NodeName\", \"Flag\"]\n",
    "    \n",
    "    node_df1 = connect_df[[\"key1\"]]\n",
    "    node_df1.columns = [\"NodeName\"]\n",
    "    node_df2 = connect_df[[\"key2\"]]\n",
    "    node_df2.columns = [\"NodeName\"]\n",
    "    node_df = pd.concat([node_df1, node_df2]).drop_duplicates().reset_index(drop=True)\n",
    "    node_df[\"NodeNum\"] = [1+i for i in range(len(node_df))]\n",
    "    node_df = pd.merge(node_df, onlyfc_gp, on=\"NodeName\", how=\"left\")\n",
    "    node_df[\"Flag\"] = node_df[\"Flag\"].fillna(0)\n",
    "\n",
    "    if image_flag:\n",
    "        img_list = []\n",
    "        for nd_i in range(len(node_df)):\n",
    "            search_word = node_df[\"NodeName\"][nd_i]\n",
    "            print(search_word)\n",
    "            img = getImageUrl(API_KEY, CUSTOM_SEARCH_ENGINE, search_word)\n",
    "            img_list.append(img)\n",
    "        node_df['ImageURL'] = img_list\n",
    "    \n",
    "    connect_df = pd.merge(connect_df, node_df, left_on=\"key1\", right_on=\"NodeName\", how=\"left\")[[\"key1\", \"key2\", \"NodeNum\", \"Flag\"]]\n",
    "    connect_df.columns = [\"key1\", \"key2\", \"NodeNum1\", \"Flag1\"]\n",
    "    connect_df = pd.merge(connect_df, node_df, left_on=\"key2\", right_on=\"NodeName\", how=\"left\")[[\"key1\", \"key2\", \"NodeNum1\", \"Flag1\", \"NodeNum\", \"Flag\"]]\n",
    "    connect_df.columns = [\"key1\", \"key2\", \"NodeNum1\", \"Flag1\", \"NodeNum2\", \"Flag2\"]\n",
    "    # connect_df[\"Flag1\"] = [1 if k1[:2] == \"FC\" else flg for flg, k1 in zip(connect_df[\"Flag1\"], connect_df[\"key1\"])]\n",
    "    # connect_df[\"Flag2\"] = [1 if k2[:2] == \"FC\" else flg for flg, k2 in zip(connect_df[\"Flag2\"], connect_df[\"key2\"])]\n",
    "    connect_df = connect_df.fillna(0)\n",
    "    connect_df[\"FCConnect\"] = connect_df[\"Flag1\"] * connect_df[\"Flag2\"]\n",
    "\n",
    "    # ネットワークのインスタンス生成\n",
    "    network = Network(\n",
    "        height=\"1000px\",  # デフォルト \"500px\"\n",
    "        width=\"2000px\",  # デフォルト \"500px\"\n",
    "        notebook=True,  # これをTrueにしておくとjupyter上で結果が見れる\n",
    "        bgcolor='#ffffff',  # 背景色。デフォルト \"#ffffff\"\n",
    "        directed=False,  # Trueにすると有向グラフ。デフォルトはFalseで無向グラフ\n",
    "    )\n",
    "\n",
    "    # add_node でノードを追加\n",
    "    for i in range(len(node_df)):\n",
    "        nd1_id = int(node_df['NodeNum'][i])\n",
    "        nd1_name = node_df['NodeName'][i]\n",
    "        nd1_flag = node_df['Flag'][i]\n",
    "\n",
    "        if nd1_flag == 1:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][0]\n",
    "        elif nd1_flag == 2:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][1]\n",
    "        elif nd1_flag == 3:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][2]\n",
    "        elif nd1_flag == 4:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][3]\n",
    "        elif nd1_flag == 5:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][4]\n",
    "        elif nd1_flag == 6:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][5]\n",
    "        elif nd1_name[:2] == \"FC\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][0]\n",
    "        elif nd1_name[:6] == \"DLData\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][1]\n",
    "        elif nd1_name[:7] == \"DLKadai\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][2]\n",
    "        elif nd1_name[:6] == \"DLCorp\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][3]\n",
    "        elif nd1_name[:6] == \"DLHojo\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][4]\n",
    "        elif nd1_name[:7] == \"UserAns\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][5]\n",
    "        else:\n",
    "            nd1_color = \"#e5c494\"\n",
    "        \n",
    "        if image_flag:\n",
    "            nd1_image = node_df['ImageURL'][i]\n",
    "            network.add_node(n_id=nd1_id, label=nd1_name, shape='image', image =nd1_image)\n",
    "        else:\n",
    "            network.add_node(n_id=nd1_id, label=nd1_name, color=nd1_color)\n",
    "\n",
    "    for i in range(len(connect_df)):\n",
    "        nd1_id = int(connect_df['NodeNum1'][i])\n",
    "        nd2_id = int(connect_df['NodeNum2'][i])\n",
    "        nd_flag = connect_df['FCConnect'][i]\n",
    "\n",
    "        if nd_flag == 1:\n",
    "            edge_color = \"#2c7bb6\"\n",
    "        else:\n",
    "            edge_color = \"#1a9641\"\n",
    "\n",
    "        # network.add_edge(nd1_id, nd2_id, color=edge_color, width = edge_width)\n",
    "        network.add_edge(nd1_id, nd2_id, color=edge_color, width=0.1)\n",
    "    # 指定したファイル名でHTMLを出力。\n",
    "    network.show(f\"D:\\\\code\\\\FCDL\\\\htmls\\\\fc_{fname}_{tp}.html\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8d350",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Clustering Model Selection</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b1ad37e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Displaying the statistical result of the Kmeans_clusteringwith preprocess method of normalization #############\n",
      "The Silhousette Score:  0.05026147047133942\n",
      "The Calinski-Harabasz Score:  3.4766760003185\n",
      "The Davies-Bouldin Score:  2.6418963298313964\n",
      "############# Displaying the statistical result of the Agglomerative_clusteringwith preprocess method of normalization #############\n",
      "The Silhousette Score:  0.05658398695627547\n",
      "The Calinski-Harabasz Score:  3.577797179762023\n",
      "The Davies-Bouldin Score:  2.6265577379925316\n"
     ]
    }
   ],
   "source": [
    "# clustering_LLM(fc_all, 4)\n",
    "clustering_Kmeans(fc_all, 4, 'normalization')\n",
    "Agglomerative_Clustering(fc_all, 4, 'normalization')\n",
    "# clustering_Kmeans(fc_all, 4, 'cosine')\n",
    "# Agglomerative_Clustering(fc_all, 4, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "72f06df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Displaying the statistical result of the Kmeans_clusteringwith preprocess method of cosine #############\n",
      "The Silhousette Score:  0.04791624169589978\n",
      "The Calinski-Harabasz Score:  3.574451415513821\n",
      "The Davies-Bouldin Score:  2.736412291821706\n",
      "############# Displaying the statistical result of the Agglomerative_clusteringwith preprocess method of cosine #############\n",
      "The Silhousette Score:  0.07201064095054989\n",
      "The Calinski-Harabasz Score:  3.233427559222141\n",
      "The Davies-Bouldin Score:  2.6706460217822654\n"
     ]
    }
   ],
   "source": [
    "clustering_Kmeans(fc_all, 4, 'cosine')\n",
    "Agglomerative_Clustering(fc_all, 4, 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398058d",
   "metadata": {},
   "source": [
    "<font color=blue size=8>Final Result</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e6e897b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Displaying the statistical result of the Kmeans_clusteringwith preprocess method of normalization #############\n",
      "The Silhousette Score:  0.05298961545833454\n",
      "The Calinski-Harabasz Score:  3.063075996826733\n",
      "The Davies-Bouldin Score:  2.5230812967117493\n",
      "250\n",
      "8101\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "D:\\code\\FCDL\\htmls\\fc_yoko_fcdl.html\n"
     ]
    }
   ],
   "source": [
    "clustering_and_create_network(fc_all, \n",
    "                              embeddings_yoko_desc, \n",
    "                              embeddings_kadai, \n",
    "                              embeddings_corp, \n",
    "                              embeddings_hojo_desc, \n",
    "                              sentences_yoko_desc_df, \n",
    "                              sentences_kadai_df, \n",
    "                              sentences_corp_df, \n",
    "                              sentences_hojo_desc_df, \n",
    "                              \"yoko\", \n",
    "                              5, \n",
    "                              25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a63e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# THE Network could not be visualized in Jupyter notebook but in a html format\n",
    "############# THE Network could not be visualized in Jupyter notebook but in a html format\n",
    "############# THE Network could not be visualized in Jupyter notebook but in a html format\n",
    "############# THE Network could not be visualized in Jupyter notebook but in a html format\n",
    "# from IPython.display import HTML\n",
    "# html_file_path = 'your_file.html'\n",
    "# HTML(filename=\"D:\\\\code\\\\FCDL\\\\htmls\\\\fc_yoko_fcdl.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
